{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPon4BivZaqV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lVGI5sDZcj8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dir_path = '/content/drive/MyDrive/Uncertainty_Quantification'\n",
        "os.chdir(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wJAkHZLZN2U"
      },
      "source": [
        "Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2DHgBnsZJJn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install evaluate\n",
        "!pip install -qqq trl==0.7.1\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7plOCteCZf9s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset, load_dataset\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWokGoGVZkEe"
      },
      "source": [
        "Create the features for stress index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YKR9r9PkDwl"
      },
      "outputs": [],
      "source": [
        "def simulate_data(num_samples):\n",
        "  # Define parameters for each distribution\n",
        "  np.random.seed(0)\n",
        "\n",
        "  # Generate synthetic data\n",
        "  data = {\n",
        "      'news_sentiment': stats.beta.rvs(a=2, b=2, loc=-1, scale=2, size=num_samples),\n",
        "      'broker_count_imbalance': stats.beta.rvs(a=2, b=2, loc=-1, scale=2, size=num_samples),\n",
        "      'volume_indicator': stats.beta.rvs(a=2, b=2, loc=-1, scale=2, size=num_samples),\n",
        "      'benchmark_price_difference': stats.beta.rvs(a=2, b=2, loc=-1, scale=2, size=num_samples),\n",
        "      'trade_count_imbalance': stats.beta.rvs(a=2, b=2, loc=-1, scale=2, size=num_samples),\n",
        "      'one_sided_trade_indicator': stats.beta.rvs(a=2, b=2, loc=-1, scale=2, size=num_samples),\n",
        "      'tranche_size_indicator': stats.beta.rvs(a=2, b=2, loc=0, scale=1, size=num_samples)\n",
        "  }\n",
        "\n",
        "  # Create DataFrame\n",
        "  df = pd.DataFrame(data)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = simulate_data(5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcuvWZ4fZuNH"
      },
      "outputs": [],
      "source": [
        "df.to_csv('./data_simulated.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data_simulated.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk9S8wa8dkAa"
      },
      "source": [
        "Created the stress index using ChatGPT and populated as a column in the above dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epq7EnAHZuuN"
      },
      "outputs": [],
      "source": [
        "df_stress_index = pd.read_csv('data_with_stress_index.csv',usecols=lambda col:col not in ['Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zLD-gMyeMFi"
      },
      "source": [
        "Creating the stress index buckets to model as a classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exfS6JFveWhN"
      },
      "outputs": [],
      "source": [
        "n_bins = 10\n",
        "bin_width = 1/n_bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rz_0VA1eBbo"
      },
      "outputs": [],
      "source": [
        "df_stress_index.loc[:,'stress_index_bucket']=pd.cut(df_stress_index['stress_index'],\\\n",
        "                                                    bins=n_bins,\\\n",
        "                                                    labels=[str(np.round(x,1))+'-'+str(np.round(x+bin_width,1)) \\\n",
        "                                                            for x in np.arange(0,1,bin_width)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjEIp36veT2Z"
      },
      "outputs": [],
      "source": [
        "df_stress_index.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJWgkHQyyhZI"
      },
      "source": [
        "Creating data for MCQ Format in Uncertainty quantification method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Bpmv1JEyfxN"
      },
      "outputs": [],
      "source": [
        "def shuffle_list(original_list):\n",
        "    # Create a copy of the list\n",
        "    shuffled_list = original_list.copy()\n",
        "\n",
        "    # Shuffle the copy\n",
        "    random.shuffle(shuffled_list)\n",
        "\n",
        "    return shuffled_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYeVVhhqygfT"
      },
      "outputs": [],
      "source": [
        "list_stress_index_bucket=[str(np.round(x,1))+'-'+str(np.round(x+bin_width,1)) for x in np.arange(0,1,bin_width)]\n",
        "list_option_choices=[chr(x) for x in range(65,65+len(list_stress_index_bucket))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMG4i5tHev7R"
      },
      "source": [
        "**Trading Bot Prompting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKRMXACke06w"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "# Prompt for the Hugging Face token\n",
        "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "import os\n",
        "os.environ['HUGGINGFACE_TOKEN'] = hf_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K82EJyEu1S6"
      },
      "outputs": [],
      "source": [
        "# model_id =  \"NousResearch/Llama-2-7b-hf\"\n",
        "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "model_id = \"microsoft/phi-2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\",token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHpop3tlvDlf"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id,token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3WTT8UCzxOe"
      },
      "source": [
        "**Prompt with 1 shot example (first row of the dataframe)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzWK3ea8ycSS"
      },
      "outputs": [],
      "source": [
        "instruction=\"\"\"\n",
        "###Instruction:\n",
        "You are fixed income corporate bond trader. Use the feature values that I will provide and select the correct option for classifying the stress index (range of 0 to 1) into one of the defined buckets.\n",
        "Feature descriptions:\n",
        "- news_sentiment: Scale from -1 (negative) to +1 (positive) reflecting the sentiment in news about the bond issuer over the past 7 days.\n",
        "- broker_count_imbalance: Difference in broker count buying or selling the same security in the last 2 days, ranging from -1 (more selling) to +1 (more buying).\n",
        "- volume_indicator: Indicates if a bond is heavily traded in the last 2 days, with -1 for high selling volume and +1 for high buying volume.\n",
        "- benchmark_price_difference: Compares a bond's quoted price to the benchmark, ranging from -1 (below benchmark) to +1 (above benchmark), indicating price stress.\n",
        "- trade_count_imbalance: Difference in buy and sell trades of a security over 2 days, from -1 (more sells) to +1 (more buys).\n",
        "- one_sided_trade_indicator: Imbalance in buy or sell trades over 7 days, with -1 for predominantly sell trades and +1 for buy trades.\n",
        "- tranche_size_indicator: Assesses bond stress by the direction of trades and tranche size, ranging from 0 (large tranche) to 1 (small tranche).\n",
        "###Question:\n",
        "Given the feature value dict as:\n",
        "{'news_sentiment': 0.3931444486731639, 'broker_count_imbalance': 0.1243619871996783, 'volume_indicator': 0.6778892697073087, 'benchmark_price_difference': 0.4977762417709135, 'trade_count_imbalance': -0.7206982745760815, 'one_sided_trade_indicator': -0.6800763028141159, 'tranche_size_indicator': 0.7248691201811168}\n",
        "Return the correct option for classifying stress index from the following options based on above features. Do not generate anything else apart from the option.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP56mDxsz2mu"
      },
      "outputs": [],
      "source": [
        "choice_prompt=''''''\n",
        "for i,j in zip(list_option_choices,shuffle_list(list_stress_index_bucket)):\n",
        "    choice_prompt+=f'({i}) {j}\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj7sAvv_z4DM"
      },
      "outputs": [],
      "source": [
        "example_prompt=''''''\n",
        "example_prompt+=instruction\n",
        "example_prompt+=choice_prompt\n",
        "example_prompt+='''\n",
        "###Answer:\n",
        "B\n",
        "'''\n",
        "##Manually added the answer from the first row of the dataframe (taken as the 1 shot example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVLtsezR0AHV"
      },
      "source": [
        "Final prompt for the model to return the stress index bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcRcgKIsz_4f"
      },
      "outputs": [],
      "source": [
        "final_prompt=''''''\n",
        "final_prompt+=example_prompt\n",
        "final_prompt+='''\n",
        "###Question:\n",
        "Given the feature value dict as:\n",
        "{'news_sentiment': 0.7724956104208642, 'broker_count_imbalance': 0.3693763241525458, 'volume_indicator': -0.8772379830243947, 'benchmark_price_difference': 0.1397481154484952, 'trade_count_imbalance': 0.8518638934191982, 'one_sided_trade_indicator': -0.2575187071551993, 'tranche_size_indicator': 0.5860627411447076}\n",
        "Return the correct option for classifying stress index from the following options based on above features. Do not generate anything else apart from the option.\n",
        "'''\n",
        "choice_prompt=''''''\n",
        "for i,j in zip(list_option_choices,shuffle_list(list_stress_index_bucket)):\n",
        "    choice_prompt+=f'({i}) {j}\\n'\n",
        "\n",
        "final_prompt+=choice_prompt\n",
        "final_prompt+='''\n",
        "###Answer:\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUiysnuk0i8u"
      },
      "source": [
        "Model Generation to see what the model predicted as next token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnzAYLwc1y5G"
      },
      "outputs": [],
      "source": [
        "def model_generation(model, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    generated_ids=model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        temperature=0.2,\n",
        "        max_new_tokens=1\n",
        "    )\n",
        "    output = tokenizer.decode(\n",
        "        generated_ids[0],\n",
        "        # stopping_criteria = [EosListStoppingCriteria()] ,\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX-o04b70rek"
      },
      "outputs": [],
      "source": [
        "output = model_generation(model, final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPbysRpA15et"
      },
      "outputs": [],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsIEcLxM16qQ"
      },
      "outputs": [],
      "source": [
        "def delete_model(model):\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOWUC_i519I0"
      },
      "outputs": [],
      "source": [
        "def delete_generation_output(output):\n",
        "    del output\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XDpZAFL0tDk"
      },
      "source": [
        "Model Forward to get the logits of the next token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu-l1AzG1tXN"
      },
      "outputs": [],
      "source": [
        "def model_forward(model, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    try:\n",
        "        outputs=model(inputs[\"input_ids\"])\n",
        "    except:\n",
        "        return\n",
        "    output_logits = outputs.logits.detach().cpu()\n",
        "    next_token_logits = output_logits[:, -1]\n",
        "    return next_token_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzUn-mMD05Fo"
      },
      "outputs": [],
      "source": [
        "##Softmax function to convert logits to probabilities\n",
        "def softmax(x):\n",
        "    e_x=np.exp(x-np.max(x))\n",
        "    return e_x/e_x.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxuwZUxr09by"
      },
      "outputs": [],
      "source": [
        "tokens_of_interest= list_option_choices #the option choices corresponding to stress index buckets\n",
        "\n",
        "token_indices = tokenizer.convert_tokens_to_ids(tokens_of_interest) #get the index for the option tokens\n",
        "\n",
        "indices_in_logits = {token: next_token_logits[0,token_idx].item() for token, token_idx in zip(tokens_of_interest, token_indices)}\n",
        "\n",
        "label_to_softmax_dict=dict(zip(tokens_of_interest,softmax(np.array(list(indices_in_logits.values())))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GSyNds-1M66"
      },
      "outputs": [],
      "source": [
        "print(label_to_softmax_dict) ##this would show the softmax for each of the labels (options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOJlmb9x1UFQ"
      },
      "source": [
        "***Run the code below to delete the variables and free up unused memory (Used when running the above model forward again to avoid CUDA out of memory issue): Note: You would to create the inputs variable again***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Q8UyIE1TC2"
      },
      "outputs": [],
      "source": [
        "# Clear memory\n",
        "def delete_inputs_outputs(inputs):\n",
        "  del inputs\n",
        "  del outputs\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gew5P9U61iY4"
      },
      "outputs": [],
      "source": [
        "def get_values_before_key(sorted_dict, key): ###updated the function to include the logits not including the true label\n",
        "    values_before_key = []\n",
        "    for k, v in sorted_dict.items():\n",
        "        if k == key:\n",
        "            # values_before_key.append(v)\n",
        "            break\n",
        "        values_before_key.append(v)\n",
        "    return values_before_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo68x2boYPeI"
      },
      "source": [
        "#### LAC CONFORMAL SCORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeZ8PpMyYNKf"
      },
      "outputs": [],
      "source": [
        "def lac(true_label, label_softmax_dict):\n",
        "    lac_score = 1.0 - label_softmax_dict[true_label]\n",
        "    return lac_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soJcA-YWYWln"
      },
      "outputs": [],
      "source": [
        "##Calculating the LAC conformal score for the example\n",
        "print(lac('H',label_to_softmax_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuCPEWkMYQOV"
      },
      "source": [
        "#### APS CONFORMAL SCORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXd_WFu0YNH3"
      },
      "outputs": [],
      "source": [
        "def aps(true_label, label_softmax_dict):\n",
        "    sorted_softmax_dict = dict(sorted(label_to_softmax_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "    high_labels = get_values_before_key(sorted_softmax_dict, true_label)\n",
        "    aps_score = sum(high_labels)\n",
        "    return aps_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82UoUPToYW8f"
      },
      "outputs": [],
      "source": [
        "print(aps('H', label_to_softmax_dict))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
